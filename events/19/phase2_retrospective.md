# Phase 2 Retrospective and Lessons Learned #

## Summary Remarks ##

In general the Dress Rehearsal was a very productive way for teams to try out their prototype solution and get familiar with the submission process for VRX.

## Results ##

The [Scoring Summary](https://bitbucket.org/osrf/vrx/wiki/VRX%202019%20Results) illustrates the scoring that was generated by each team's solution.  Each team's submission was run against 36 different simulation scenarios (6 tasks and 6 trials of each task).  


## Practice Worlds and Competition Worlds ##

In preparation for Phase 2 we released 3 trials for each of the 6 tasks - a total of 18 practice simulations for teams to test their solutions: [Phase 2 Task Testing](https://bitbucket.org/osrf/vrx/wiki/Phase2_Task_Testing_2019).  In running the Phase 2 Dress Rehearsal we executed the submitted code against 6 trials for each task; the first 3 trials were equivalent to the practice trials and 3 new trials that were different than the original 3 practice trials.

After the Phase 2 deadline, all of the worlds and models used for the 36 trials (6 tasks, 6 trials each) have been made a part of the VRX repository (see [PR#207](https://bitbucket.org/osrf/vrx/pull-requests/207/adding-worlds-and-models-used-in-phase2/diff)).  Following the directions in [Phase 2 Task Testing](https://bitbucket.org/osrf/vrx/wiki/Phase2_Task_Testing_2019) teams can use these trials for testing their code in many different environmental conditions.  See descriptions of the [Phase 2 Trials](https://bitbucket.org/osrf/vrx/wiki/events/19/phase2_trials).

## P3D Gazebo Plugin ##

Going through the submissions for Phase 2, we noticed that the teams include the P3D plugin in their sensor_config.yaml file. The P3D plugin provides a ROS published ground truth - true, noiseless pose information from Gazebo.  During development it is often handy to know the true state of the system, but the system should estimate the state of the system from sensors - not use the ground truth directly in the solution.  

This "sensor" is not included in the Technical Guide, but the [Sensor Configuration Wiki ](https://bitbucket.org/osrf/vrx/wiki/tutorials/Creating%20a%20custom%20WAM-V%20Thruster%20and%20Sensor%20Configuration%20For%20Competition) points to the `numeric.yaml` file which could be interpreted to suggest that a sensor configuration that includes one P3D sensor is compliant.

For Phase 3 including the P3D sensor in the `sensor_config.yaml` file will result in a non-compliant configuration.

## Dock and Scan-And-Dock Task Names ##

The Task Naming in the VRX Competition and Task Descriptions document describe how the six individuals tasks (stationkeeping, wayfinding, etc.) are announced via the ROS interface.  In that document the "Dock" task corresponds to a [Task Msg](https://bitbucket.org/osrf/vrx/src/default/vrx_gazebo/msg/Task.msg) name of "scan", and the "Scan and Dock" task corresponds to a Task Msg with the name "scan_and_dock".  There were two potential issues with this during Phase 2.

First, there was a bug in the practice words released prior to Phase 2: [Phase 2 Task Testing](https://bitbucket.org/osrf/vrx/wiki/Phase2_Task_Testing_2019).  This bug caused Task Messages both the "Dock" and "Scan and Dock" tasks to have the name "scan_dock".  This was corrected with during evaluation of the Phase 2 submissions so that the task names were "scan" and "scan_and_dock" as specified in the competition documents.  These trials, with the correct names, have been released - see [PR#207](https://bitbucket.org/osrf/vrx/pull-requests/207/adding-worlds-and-models-used-in-phase2/diff).


Second, the naming may have caused some confusion, since we probably intended to use use the Task Msg name "dock" to indicate this "Dock" task.  However, we feel it is important to stick to the documentation as written which specifies the Task Msg for the "Dock" task as "scan".  

For Phase 3 we will not be changing the interface, so the Dock Task (Task 5) will continue to be announced in the Task Message using the name "scan".

## Team Docker entry points ##

While running the Phase 2 submissions, some of the team Docker images were not properly configured to run the [vrx-docker scripts](https://bitbucket.org/osrf/vrx-docker/src/default/). In particular, the main problem was that the team image was not executing its expected entry point. Some teams were relying on a  `.bashrc` that is parsed when running docker with the `-it` flag. However, the VRX `run_trial.bash` script is not using that option. See [here](https://bitbucket.org/osrf/vrx-docker/src/45323a4bbf7fe63d6a087fac44d710672f5ea771/run_trial.bash#lines-160) how we use `docker run` with your team image. Check out this tutorial for tips and instructions on how to create your Docker images.

We contacted most of the teams while running Phase 2, but keep in mind that is your responsibility to guarantee that your image is properly configured for Phase 3.